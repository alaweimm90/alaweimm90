groups:
  - name: atlas_service_alerts
    rules:
      # API Gateway alerts
      - alert: AtlasApiGatewayDown
        expr: up{job="atlas-api-gateway"} == 0
        for: 5m
        labels:
          severity: critical
          service: api-gateway
        annotations:
          summary: "ATLAS API Gateway is down"
          description: "ATLAS API Gateway has been down for more than 5 minutes."

      - alert: AtlasApiGatewayHighErrorRate
        expr: rate(http_requests_total{job="atlas-api-gateway", status=~"5.."}[5m]) / rate(http_requests_total{job="atlas-api-gateway"}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          service: api-gateway
        annotations:
          summary: "High error rate on ATLAS API Gateway"
          description: "ATLAS API Gateway error rate is {{ $value | printf "%.2f" }}% over the last 5 minutes."

      - alert: AtlasApiGatewayHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="atlas-api-gateway"}[5m])) > 2
        for: 5m
        labels:
          severity: warning
          service: api-gateway
        annotations:
          summary: "High latency on ATLAS API Gateway"
          description: "ATLAS API Gateway 95th percentile latency is {{ $value | printf "%.2f" }}s over the last 5 minutes."

      # Orchestration Service alerts
      - alert: AtlasOrchestrationDown
        expr: up{job="atlas-orchestration"} == 0
        for: 5m
        labels:
          severity: critical
          service: orchestration
        annotations:
          summary: "ATLAS Orchestration Service is down"
          description: "ATLAS Orchestration Service has been down for more than 5 minutes."

      - alert: AtlasOrchestrationHighQueueSize
        expr: atlas_task_queue_size > 100
        for: 5m
        labels:
          severity: warning
          service: orchestration
        annotations:
          summary: "High task queue size in ATLAS Orchestration"
          description: "ATLAS Orchestration task queue size is {{ $value }} tasks."

      # Execution Service alerts
      - alert: AtlasExecutionDown
        expr: up{job="atlas-execution"} == 0
        for: 5m
        labels:
          severity: critical
          service: execution
        annotations:
          summary: "ATLAS Execution Service is down"
          description: "ATLAS Execution Service has been down for more than 5 minutes."

      - alert: AtlasExecutionHighErrorRate
        expr: rate(atlas_task_execution_errors_total[5m]) / rate(atlas_task_executions_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: execution
        annotations:
          summary: "High task execution error rate"
          description: "ATLAS Execution error rate is {{ $value | printf "%.2f" }}% over the last 5 minutes."

      # AI Provider alerts
      - alert: AtlasAIProviderRateLimit
        expr: rate(atlas_ai_provider_rate_limit_hits_total[5m]) > 10
        for: 2m
        labels:
          severity: warning
          service: execution
        annotations:
          summary: "AI Provider rate limit hit"
          description: "ATLAS hit AI provider rate limits {{ $value | printf "%.0f" }} times in the last 5 minutes."

      # Storage Service alerts
      - alert: AtlasStorageDown
        expr: up{job="atlas-storage"} == 0
        for: 5m
        labels:
          severity: critical
          service: storage
        annotations:
          summary: "ATLAS Storage Service is down"
          description: "ATLAS Storage Service has been down for more than 5 minutes."

      - alert: AtlasStorageHighLatency
        expr: histogram_quantile(0.95, rate(atlas_storage_operation_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
          service: storage
        annotations:
          summary: "High storage operation latency"
          description: "ATLAS Storage 95th percentile latency is {{ $value | printf "%.2f" }}s over the last 5 minutes."

      # Optimization Service alerts
      - alert: AtlasOptimizationDown
        expr: up{job="atlas-optimization"} == 0
        for: 10m
        labels:
          severity: warning
          service: optimization
        annotations:
          summary: "ATLAS Optimization Service is down"
          description: "ATLAS Optimization Service has been down for more than 10 minutes."

      - alert: AtlasOptimizationFailedRefactorings
        expr: rate(atlas_refactoring_failures_total[1h]) > 5
        for: 15m
        labels:
          severity: warning
          service: optimization
        annotations:
          summary: "High refactoring failure rate"
          description: "ATLAS Optimization failed {{ $value | printf "%.0f" }} refactorings in the last hour."

  - name: infrastructure_alerts
    rules:
      # Kubernetes alerts
      - alert: KubernetesPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total{namespace="atlas-system"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Pod crash looping in ATLAS namespace"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping."

      - alert: KubernetesHighCPUUsage
        expr: rate(container_cpu_usage_seconds_total{namespace="atlas-system"}[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage in ATLAS namespace"
          description: "Container {{ $labels.container }} in pod {{ $labels.pod }} has high CPU usage."

      - alert: KubernetesHighMemoryUsage
        expr: container_memory_usage_bytes{namespace="atlas-system"} / container_spec_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage in ATLAS namespace"
          description: "Container {{ $labels.container }} in pod {{ $labels.pod }} is using {{ $value | printf "%.0f" }}% of memory limit."

      # Database alerts
      - alert: PostgresDown
        expr: up{job="postgres"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL database is down"
          description: "PostgreSQL database has been down for more than 5 minutes."

      - alert: PostgresHighConnections
        expr: pg_stat_activity_count{datname="atlas"} > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High PostgreSQL connection count"
          description: "PostgreSQL has {{ $value }} active connections."

      # Redis alerts
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Redis cache is down"
          description: "Redis cache has been down for more than 5 minutes."

      - alert: RedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Redis memory usage"
          description: "Redis is using {{ $value | printf "%.0f" }}% of available memory."

      # Elasticsearch alerts
      - alert: ElasticsearchDown
        expr: up{job="elasticsearch"} == 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Elasticsearch is down"
          description: "Elasticsearch has been down for more than 10 minutes."

      - alert: ElasticsearchHighCPUUsage
        expr: rate(elasticsearch_process_cpu_percent[5m]) > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Elasticsearch CPU usage"
          description: "Elasticsearch CPU usage is {{ $value | printf "%.0f" }}%."