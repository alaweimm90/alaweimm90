{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "# TODO: Add remaining concepts for complete tutorial\n# Still need:\n# 4. Bandgap Prediction - Use descriptors to predict electronic properties  \n# 5. Graph Neural Networks - Implement basic GNN for crystal structures\n# 6. Materials Applications - Real-world examples with actual datasets\n# \n# RESEARCH NOTES:\n# - Check CGCNN paper for graph representation ideas\n# - Look up JARVIS database for training data\n# - Consider using PyTorch Geometric for GNN implementation\n#\n# BOOKMARK: https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301\n\nprint(\"üöß PyTorch tutorial - 3/6 concepts completed\")\nprint(\"Next: Work on bandgap prediction using crystal descriptors\")\nprint(\"Need to find good dataset - maybe start with simple binary compounds?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Crystal descriptor generation and feature engineering\nimport re\nfrom collections import Counter\n\ndef parse_chemical_formula(formula):\n    \"\"\"Parse chemical formula and extract elemental composition.\"\"\"\n    # Simple regex pattern for elements and counts\n    pattern = r'([A-Z][a-z]?)(\\d*)'\n    matches = re.findall(pattern, formula)\n    \n    composition = {}\n    for element, count in matches:\n        count = int(count) if count else 1\n        composition[element] = composition.get(element, 0) + count\n    \n    return composition\n\ndef get_atomic_properties():\n    \"\"\"Return basic atomic properties for common elements.\"\"\"\n    # Simplified atomic properties database\n    atomic_data = {\n        'H': {'number': 1, 'mass': 1.008, 'radius': 0.37, 'electronegativity': 2.20},\n        'Li': {'number': 3, 'mass': 6.941, 'radius': 1.52, 'electronegativity': 0.98},\n        'C': {'number': 6, 'mass': 12.011, 'radius': 0.77, 'electronegativity': 2.55},\n        'N': {'number': 7, 'mass': 14.007, 'radius': 0.75, 'electronegativity': 3.04},\n        'O': {'number': 8, 'mass': 15.999, 'radius': 0.73, 'electronegativity': 3.44},\n        'Si': {'number': 14, 'mass': 28.086, 'radius': 1.18, 'electronegativity': 1.90},\n        'P': {'number': 15, 'mass': 30.974, 'radius': 1.10, 'electronegativity': 2.19},\n        'S': {'number': 16, 'mass': 32.065, 'radius': 1.02, 'electronegativity': 2.58},\n        'Ga': {'number': 31, 'mass': 69.723, 'radius': 1.36, 'electronegativity': 1.81},\n        'As': {'number': 33, 'mass': 74.922, 'radius': 1.19, 'electronegativity': 2.18},\n        'Se': {'number': 34, 'mass': 78.971, 'radius': 1.16, 'electronegativity': 2.55},\n        'Mo': {'number': 42, 'mass': 95.95, 'radius': 1.39, 'electronegativity': 2.16},\n        'Cd': {'number': 48, 'mass': 112.411, 'radius': 1.58, 'electronegativity': 1.69},\n        'In': {'number': 49, 'mass': 114.818, 'radius': 1.67, 'electronegativity': 1.78},\n        'Te': {'number': 52, 'mass': 127.60, 'radius': 1.35, 'electronegativity': 2.1},\n        'W': {'number': 74, 'mass': 183.84, 'radius': 1.46, 'electronegativity': 2.36},\n        'B': {'number': 5, 'mass': 10.811, 'radius': 0.85, 'electronegativity': 2.04},\n        'Al': {'number': 13, 'mass': 26.982, 'radius': 1.43, 'electronegativity': 1.61},\n        'Ti': {'number': 22, 'mass': 47.867, 'radius': 1.47, 'electronegativity': 1.54},\n        'Zn': {'number': 30, 'mass': 65.38, 'radius': 1.39, 'electronegativity': 1.65},\n        'Sn': {'number': 50, 'mass': 118.71, 'radius': 1.72, 'electronegativity': 1.96},\n        'Cu': {'number': 29, 'mass': 63.546, 'radius': 1.32, 'electronegativity': 1.90}\n    }\n    return atomic_data\n\ndef calculate_crystal_descriptors(formula, lattice_a, lattice_b, lattice_c):\n    \"\"\"Calculate comprehensive crystal descriptors from formula and structure.\"\"\"\n    \n    # Parse composition\n    composition = parse_chemical_formula(formula)\n    atomic_data = get_atomic_properties()\n    \n    # Calculate total atoms and normalize composition\n    total_atoms = sum(composition.values())\n    normalized_comp = {el: count/total_atoms for el, count in composition.items()}\n    \n    descriptors = {}\n    \n    # Composition descriptors\n    mass_sum = sum(normalized_comp[el] * atomic_data[el]['mass'] for el in composition if el in atomic_data)\n    descriptors['avg_atomic_mass'] = mass_sum\n    \n    atomic_numbers = [atomic_data[el]['number'] for el in composition if el in atomic_data]\n    descriptors['avg_atomic_number'] = sum(normalized_comp[el] * atomic_data[el]['number'] \n                                          for el in composition if el in atomic_data)\n    \n    electronegativities = [atomic_data[el]['electronegativity'] for el in composition if el in atomic_data]\n    descriptors['avg_electronegativity'] = sum(normalized_comp[el] * atomic_data[el]['electronegativity'] \n                                              for el in composition if el in atomic_data)\n    \n    radii = [atomic_data[el]['radius'] for el in composition if el in atomic_data]\n    descriptors['avg_atomic_radius'] = sum(normalized_comp[el] * atomic_data[el]['radius'] \n                                          for el in composition if el in atomic_data)\n    \n    # Structure descriptors\n    descriptors['lattice_a'] = lattice_a\n    descriptors['lattice_b'] = lattice_b\n    descriptors['lattice_c'] = lattice_c\n    descriptors['volume'] = lattice_a * lattice_b * lattice_c\n    descriptors['avg_lattice'] = (lattice_a + lattice_b + lattice_c) / 3\n    \n    # Anisotropy measures\n    lattice_params = [lattice_a, lattice_b, lattice_c]\n    descriptors['lattice_anisotropy'] = (max(lattice_params) - min(lattice_params)) / np.mean(lattice_params)\n    \n    # Binary features\n    descriptors['is_binary'] = float(len(composition) == 2)\n    descriptors['is_ternary'] = float(len(composition) == 3)\n    descriptors['contains_transition_metal'] = float(any(atomic_data[el]['number'] >= 21 and atomic_data[el]['number'] <= 30 \n                                                        for el in composition if el in atomic_data))\n    \n    return descriptors\n\n# Example: Generate descriptors for real materials\nprint(\"üî¨ Crystal Descriptor Generation\")\nprint(\"=\" * 40)\n\n# Real materials data\nmaterials_examples = [\n    {\"name\": \"Silicon\", \"formula\": \"Si\", \"a\": 5.431, \"b\": 5.431, \"c\": 5.431, \"bandgap\": 1.12},\n    {\"name\": \"GaAs\", \"formula\": \"GaAs\", \"a\": 5.653, \"b\": 5.653, \"c\": 5.653, \"bandgap\": 1.42},\n    {\"name\": \"MoS2\", \"formula\": \"MoS2\", \"a\": 3.16, \"b\": 3.16, \"c\": 12.30, \"bandgap\": 1.80},\n    {\"name\": \"CdTe\", \"formula\": \"CdTe\", \"a\": 6.482, \"b\": 6.482, \"c\": 6.482, \"bandgap\": 1.50}\n]\n\n# Calculate descriptors for each material\ndescriptor_data = []\nfor material in materials_examples:\n    descriptors = calculate_crystal_descriptors(\n        material[\"formula\"], material[\"a\"], material[\"b\"], material[\"c\"]\n    )\n    descriptors[\"material\"] = material[\"name\"]\n    descriptors[\"bandgap\"] = material[\"bandgap\"]\n    descriptor_data.append(descriptors)\n\n# Convert to DataFrame for analysis\ndf_descriptors = pd.DataFrame(descriptor_data)\nfeature_columns = [col for col in df_descriptors.columns if col not in ['material', 'bandgap']]\n\nprint(f\"Generated {len(feature_columns)} crystal descriptors:\")\nfor i, col in enumerate(feature_columns, 1):\n    print(f\"{i:2d}. {col}\")\n\nprint(f\"\\nüìä Descriptor Values for Example Materials:\")\ndisplay_df = df_descriptors[['material'] + feature_columns[:6]].round(3)\nprint(display_df.to_string(index=False))\n\n# Correlation analysis between descriptors and bandgap\nprint(f\"\\nüîó Descriptor-Bandgap Correlations:\")\ncorrelations = df_descriptors[feature_columns + ['bandgap']].corr()['bandgap'].abs().sort_values(ascending=False)\nfor descriptor, corr in correlations.head(8).items():\n    if descriptor != 'bandgap':\n        print(f\"{descriptor:25s}: {corr:.3f}\")\n\n# Convert descriptors to PyTorch tensors for neural network input\nX_descriptors = torch.FloatTensor(df_descriptors[feature_columns].values)\ny_bandgaps = torch.FloatTensor(df_descriptors['bandgap'].values)\n\nprint(f\"\\nüéØ Descriptor Tensor Shape: {X_descriptors.shape}\")\nprint(f\"Features per material: {X_descriptors.shape[1]}\")\nprint(f\"Materials: {X_descriptors.shape[0]}\")\n\n# Visualization of key descriptors\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Average atomic number vs bandgap\nax1.scatter(df_descriptors['avg_atomic_number'], df_descriptors['bandgap'], \n           s=100, c='blue', alpha=0.7, edgecolors='black')\nax1.set_xlabel('Average Atomic Number')\nax1.set_ylabel('Bandgap (eV)')\nax1.set_title('Atomic Number vs Bandgap')\nax1.grid(True, alpha=0.3)\nfor i, row in df_descriptors.iterrows():\n    ax1.annotate(row['material'], (row['avg_atomic_number'], row['bandgap']), \n                xytext=(5, 5), textcoords='offset points')\n\n# Plot 2: Volume vs bandgap\nax2.scatter(df_descriptors['volume'], df_descriptors['bandgap'], \n           s=100, c='red', alpha=0.7, edgecolors='black')\nax2.set_xlabel('Unit Cell Volume (≈≥)')\nax2.set_ylabel('Bandgap (eV)')\nax2.set_title('Volume vs Bandgap')\nax2.grid(True, alpha=0.3)\nfor i, row in df_descriptors.iterrows():\n    ax2.annotate(row['material'], (row['volume'], row['bandgap']), \n                xytext=(5, 5), textcoords='offset points')\n\n# Plot 3: Electronegativity vs bandgap\nax3.scatter(df_descriptors['avg_electronegativity'], df_descriptors['bandgap'], \n           s=100, c='green', alpha=0.7, edgecolors='black')\nax3.set_xlabel('Average Electronegativity')\nax3.set_ylabel('Bandgap (eV)')\nax3.set_title('Electronegativity vs Bandgap')\nax3.grid(True, alpha=0.3)\nfor i, row in df_descriptors.iterrows():\n    ax3.annotate(row['material'], (row['avg_electronegativity'], row['bandgap']), \n                xytext=(5, 5), textcoords='offset points')\n\n# Plot 4: Descriptor correlation matrix\ncorrelation_matrix = df_descriptors[feature_columns[:8]].corr()\nim = ax4.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\nax4.set_xticks(range(len(correlation_matrix.columns)))\nax4.set_yticks(range(len(correlation_matrix.columns)))\nax4.set_xticklabels([col[:10] + '...' if len(col) > 10 else col for col in correlation_matrix.columns], rotation=45, ha='right')\nax4.set_yticklabels([col[:10] + '...' if len(col) > 10 else col for col in correlation_matrix.columns])\nax4.set_title('Descriptor Correlation Matrix')\nplt.colorbar(im, ax=ax4, fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° Crystal Descriptor Insights:\")\nprint(\"‚Ä¢ Descriptors capture both compositional and structural information\")\nprint(\"‚Ä¢ Different materials show distinct fingerprints in descriptor space\")\nprint(\"‚Ä¢ Strong correlations exist between certain descriptors and properties\")\nprint(\"‚Ä¢ These features can be used as input to machine learning models\")\nprint(\"‚Ä¢ Feature engineering is crucial for materials informatics success\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Crystal Descriptors - Feature Engineering for Materials\n\nCrystal descriptors are numerical representations that capture the essential structural and chemical features of materials. Proper feature engineering is critical for successful machine learning in materials science.\n\n## Types of Crystal Descriptors\n\n**Composition-based descriptors:**\n- Stoichiometric features: elemental ratios, valence electron count\n- Statistical features: mean atomic mass, electronegativity differences\n- Orbital features: s/p/d electron counts\n\n**Structure-based descriptors:**\n- Geometric features: lattice parameters, volume, coordination numbers\n- Symmetry features: space group, point group operations\n- Topological features: ring statistics, void fractions\n\n**Property-based descriptors:**\n- Atomic properties: ionic radii, electronegativity, polarizability\n- Bulk properties: cohesive energy, bulk modulus estimates\n\nThis section demonstrates creating and using crystal descriptors for materials prediction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_cell"
   },
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# üì¶ Install PyTorch (Colab only)\n",
    "# ==========================\n",
    "!pip install torch torchvision torch-geometric matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_cell"
   },
   "source": [
    "# Deep Learning with PyTorch: A Beginner's Guide\n",
    "\n",
    "This notebook introduces fundamental concepts in deep learning using PyTorch, specifically for materials science applications. We will explore tensor operations, neural network architectures, crystal descriptors, property prediction, graph neural networks, and real materials applications.\n",
    "\n",
    "**Learning Path**: Tensors & Autograd ‚Üí Neural Networks ‚Üí Crystal Descriptors ‚Üí Bandgap Prediction ‚Üí Graph Neural Networks ‚Üí Materials Applications\n",
    "\n",
    "Let's start by setting up our deep learning environment for materials science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_cell"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for deep learning and materials science\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"üéØ Deep learning environment ready for materials science!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "concept_1_theory"
   },
   "source": [
    "# ‚úÖ 1. Tensors & Autograd - Deep Learning Foundations\n",
    "\n",
    "PyTorch tensors are the fundamental data structure for deep learning, similar to NumPy arrays but with additional capabilities for automatic differentiation (autograd). Understanding tensors and gradients is essential for materials property prediction.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "A tensor is a generalization of vectors and matrices to higher dimensions:\n",
    "- **Scalar** (0D): $s$\n",
    "- **Vector** (1D): $\\mathbf{v} = [v_1, v_2, ..., v_n]$\n",
    "- **Matrix** (2D): $\\mathbf{M} = \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix}$\n",
    "- **Tensor** (nD): Multidimensional array\n",
    "\n",
    "**Automatic differentiation** computes gradients via chain rule:\n",
    "$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$\n",
    "\n",
    "## Applications in Materials Science\n",
    "\n",
    "Tensors represent:\n",
    "- **Crystal structures**: Atomic positions and lattice parameters\n",
    "- **Material properties**: Bandgaps, formation energies, elastic constants\n",
    "- **Feature vectors**: Descriptors for machine learning models\n",
    "\n",
    "This section demonstrates tensor operations essential for materials informatics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "concept_1_code"
   },
   "outputs": [],
   "source": [
    "# Basic tensor operations for materials data\n",
    "print(\"üîß Tensor Operations for Materials Science\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create tensors representing material properties\n",
    "# Example: Bandgaps for different materials (in eV)\n",
    "bandgaps = torch.tensor([1.12, 1.42, 1.8, 1.6, 0.0, 5.9], dtype=torch.float32)\n",
    "materials = [\"Si\", \"GaAs\", \"MoS2\", \"WSe2\", \"Graphene\", \"hBN\"]\n",
    "\n",
    "print(f\"Material bandgaps (eV): {bandgaps}\")\n",
    "print(f\"Materials: {materials}\")\n",
    "print(f\"Tensor shape: {bandgaps.shape}\")\n",
    "print(f\"Data type: {bandgaps.dtype}\")\n",
    "\n",
    "# Statistical operations useful for materials analysis\n",
    "print(f\"\\nüìä Statistical Analysis:\")\n",
    "print(f\"Mean bandgap: {torch.mean(bandgaps):.2f} eV\")\n",
    "print(f\"Standard deviation: {torch.std(bandgaps):.2f} eV\")\n",
    "print(f\"Min/Max: {torch.min(bandgaps):.1f} / {torch.max(bandgaps):.1f} eV\")\n",
    "\n",
    "# 2D tensor: Crystal structure data (simplified)\n",
    "# Rows: materials, Columns: [lattice_a, lattice_b, lattice_c, bandgap]\n",
    "crystal_data = torch.tensor([\n",
    "    [5.431, 5.431, 5.431, 1.12],  # Silicon\n",
    "    [5.653, 5.653, 5.653, 1.42],  # GaAs\n",
    "    [3.160, 3.160, 12.30, 1.80],  # MoS2\n",
    "    [3.280, 3.280, 12.96, 1.60],  # WSe2\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\nüî¨ Crystal Structure Data:\")\n",
    "print(f\"Shape: {crystal_data.shape} (4 materials √ó 4 properties)\")\n",
    "print(f\"Data:\\n{crystal_data}\")\n",
    "\n",
    "# Tensor indexing and slicing\n",
    "lattice_parameters = crystal_data[:, :3]  # First 3 columns (lattice params)\n",
    "bandgaps_2d = crystal_data[:, 3]          # Last column (bandgaps)\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Lattice parameters:\\n{lattice_parameters}\")\n",
    "print(f\"\\n‚ö° Extracted bandgaps: {bandgaps_2d}\")\n",
    "\n",
    "# Demonstrate automatic differentiation (autograd)\n",
    "print(f\"\\nüéØ Automatic Differentiation Example\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create tensor with gradient tracking enabled\n",
    "lattice_a = torch.tensor([5.0], requires_grad=True)\n",
    "\n",
    "# Simple model: volume depends on lattice parameter\n",
    "# V = a¬≥ (cubic crystal)\n",
    "volume = lattice_a ** 3\n",
    "\n",
    "print(f\"Lattice parameter a = {lattice_a.item():.2f} √Ö\")\n",
    "print(f\"Volume V = a¬≥ = {volume.item():.2f} ≈≥\")\n",
    "\n",
    "# Compute gradient dV/da = 3a¬≤\n",
    "volume.backward()\n",
    "gradient = lattice_a.grad\n",
    "\n",
    "print(f\"Gradient dV/da = {gradient.item():.2f}\")\n",
    "print(f\"Analytical: dV/da = 3a¬≤ = 3 √ó {lattice_a.item()}¬≤ = {3 * lattice_a.item()**2:.2f}\")\n",
    "print(\"‚úÖ Automatic differentiation matches analytical result!\")\n",
    "\n",
    "# Visualization of tensor operations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Bandgap distribution\n",
    "ax1.bar(materials, bandgaps.numpy(), color='steelblue', alpha=0.7)\n",
    "ax1.set_ylabel('Bandgap (eV)')\n",
    "ax1.set_title('Materials Bandgap Distribution')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Lattice parameter vs bandgap correlation\n",
    "lattice_avg = torch.mean(lattice_parameters, dim=1)  # Average lattice parameter\n",
    "ax2.scatter(lattice_avg.numpy(), bandgaps_2d.numpy(), \n",
    "           s=100, c='red', alpha=0.7, edgecolors='black')\n",
    "ax2.set_xlabel('Average Lattice Parameter (√Ö)')\n",
    "ax2.set_ylabel('Bandgap (eV)')\n",
    "ax2.set_title('Structure-Property Relationship')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add material labels\n",
    "for i, material in enumerate([\"Si\", \"GaAs\", \"MoS2\", \"WSe2\"]):\n",
    "    ax2.annotate(material, (lattice_avg[i], bandgaps_2d[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Learning Points:\")\n",
    "print(\"‚Ä¢ Tensors efficiently represent materials data\")\n",
    "print(\"‚Ä¢ Autograd enables gradient-based optimization\")\n",
    "print(\"‚Ä¢ Tensor operations reveal structure-property relationships\")\n",
    "print(\"‚Ä¢ This foundation enables neural network training for materials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "concept_2_theory"
   },
   "source": [
    "# ‚úÖ 2. Neural Networks - Basic Architectures for Materials\n",
    "\n",
    "Neural networks are powerful function approximators that can learn complex structure-property relationships in materials. Understanding basic architectures is crucial for applying deep learning to materials informatics.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "A feedforward neural network with one hidden layer:\n",
    "\n",
    "$\\mathbf{h} = \\sigma(\\mathbf{W_1} \\mathbf{x} + \\mathbf{b_1})$\n",
    "\n",
    "$\\mathbf{y} = \\mathbf{W_2} \\mathbf{h} + \\mathbf{b_2}$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$: Input features (material descriptors)\n",
    "- $\\mathbf{W_1}, \\mathbf{b_1}$: Hidden layer weights and biases\n",
    "- $\\sigma$: Activation function (ReLU, sigmoid, tanh)\n",
    "- $\\mathbf{y}$: Output prediction (material property)\n",
    "\n",
    "**Loss function** for regression:\n",
    "$L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$ (Mean Squared Error)\n",
    "\n",
    "## Applications in Materials Science\n",
    "\n",
    "Neural networks can predict:\n",
    "- **Electronic properties**: Bandgaps, work functions, DOS\n",
    "- **Mechanical properties**: Bulk modulus, hardness, elasticity\n",
    "- **Thermodynamic properties**: Formation energies, phase stability\n",
    "\n",
    "This section demonstrates building neural networks for materials property prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "concept_2_code"
   },
   "outputs": [],
   "source": [
    "# Define a simple neural network for materials property prediction\n",
    "class MaterialsNN(nn.Module):\n",
    "    \"\"\"Simple neural network for predicting material properties.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MaterialsNN, self).__init__()\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)    # Input to hidden\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)   # Hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)   # Hidden to output\n",
    "        self.dropout = nn.Dropout(0.1)                   # Regularization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = F.relu(self.fc1(x))      # First hidden layer with ReLU activation\n",
    "        x = self.dropout(x)          # Apply dropout for regularization\n",
    "        x = F.relu(self.fc2(x))      # Second hidden layer\n",
    "        x = self.fc3(x)              # Output layer (no activation for regression)\n",
    "        return x\n",
    "\n",
    "# Create synthetic materials dataset for demonstration\n",
    "def generate_materials_dataset(n_samples=1000):\n",
    "    \"\"\"Generate synthetic materials dataset for neural network training.\"\"\"\n",
    "    \n",
    "    # Features: [atomic_number_avg, lattice_parameter, density, n_electrons]\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    atomic_number = np.random.uniform(10, 80, n_samples)  # Average atomic number\n",
    "    lattice_param = np.random.uniform(3.0, 8.0, n_samples)  # Lattice parameter (√Ö)\n",
    "    density = np.random.uniform(2.0, 15.0, n_samples)    # Density (g/cm¬≥)\n",
    "    n_electrons = np.random.uniform(10, 100, n_samples)   # Number of valence electrons\n",
    "    \n",
    "    # Stack features\n",
    "    X = np.column_stack([atomic_number, lattice_param, density, n_electrons])\n",
    "    \n",
    "    # Target: Synthetic bandgap with realistic trends\n",
    "    # Bandgap tends to decrease with atomic number, increase with lattice parameter\n",
    "    bandgap = (5.0 - 0.03 * atomic_number + \n",
    "               0.2 * lattice_param - \n",
    "               0.05 * density + \n",
    "               0.01 * n_electrons + \n",
    "               np.random.normal(0, 0.3, n_samples))  # Add noise\n",
    "    \n",
    "    # Ensure realistic bandgap range (0 to 6 eV)\n",
    "    bandgap = np.clip(bandgap, 0, 6)\n",
    "    \n",
    "    return X, bandgap\n",
    "\n",
    "# Generate training data\n",
    "print(\"üî¨ Generating Synthetic Materials Dataset\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "X, y = generate_materials_dataset(n_samples=1000)\n",
    "feature_names = ['Atomic Number', 'Lattice Param (√Ö)', 'Density (g/cm¬≥)', 'N Electrons']\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Target: Bandgap (eV)\")\n",
    "print(f\"Bandgap range: {y.min():.2f} - {y.max():.2f} eV\")\n",
    "\n",
    "# Show sample data\n",
    "df_sample = pd.DataFrame(X[:5], columns=feature_names)\n",
    "df_sample['Bandgap (eV)'] = y[:5]\n",
    "print(f\"\\nüìä Sample Data:\")\n",
    "print(df_sample.round(2))\n",
    "\n",
    "# Split data and prepare for PyTorch\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "print(f\"\\nüéØ Training set: {X_train_tensor.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_tensor.shape[0]} samples\")\n",
    "\n",
    "# Initialize neural network\n",
    "input_size = X.shape[1]  # Number of features\n",
    "hidden_size = 64         # Hidden layer size\n",
    "output_size = 1          # Predict single value (bandgap)\n",
    "\n",
    "model = MaterialsNN(input_size, hidden_size, output_size)\n",
    "print(f\"\\nüß† Neural Network Architecture:\")\n",
    "print(f\"Input size: {input_size} features\")\n",
    "print(f\"Hidden layers: 2 √ó {hidden_size} neurons\")\n",
    "print(f\"Output size: {output_size} (bandgap prediction)\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Training Setup:\")\n",
    "print(f\"Loss function: Mean Squared Error\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"Activation: ReLU\")\n",
    "print(f\"Regularization: Dropout (10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "concept_2_training"
   },
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "print(\"üöÄ Training Neural Network for Materials Property Prediction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()                    # Clear gradients\n",
    "    outputs = model(X_train_tensor)          # Predict bandgaps\n",
    "    loss = criterion(outputs.squeeze(), y_train_tensor)  # Compute loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()                          # Compute gradients\n",
    "    optimizer.step()                         # Update weights\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs.squeeze(), y_test_tensor)\n",
    "    model.train()\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training completed!\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Training predictions\n",
    "    train_pred = model(X_train_tensor).squeeze().numpy()\n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    \n",
    "    # Test predictions\n",
    "    test_pred = model(X_test_tensor).squeeze().numpy()\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "print(f\"\\nüìä Model Performance:\")\n",
    "print(f\"Training - MAE: {train_mae:.3f} eV, R¬≤: {train_r2:.3f}\")\n",
    "print(f\"Test - MAE: {test_mae:.3f} eV, R¬≤: {test_r2:.3f}\")\n",
    "\n",
    "# Visualization of training progress and results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Training curves\n",
    "ax1.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
    "ax1.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Mean Squared Error')\n",
    "ax1.set_title('Neural Network Training Progress')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training predictions\n",
    "ax2.scatter(y_train, train_pred, alpha=0.5, s=20, color='blue', label='Training')\n",
    "ax2.plot([0, 6], [0, 6], 'r--', label='Perfect Prediction')\n",
    "ax2.set_xlabel('True Bandgap (eV)')\n",
    "ax2.set_ylabel('Predicted Bandgap (eV)')\n",
    "ax2.set_title(f'Training Predictions (R¬≤ = {train_r2:.3f})')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Test predictions\n",
    "ax3.scatter(y_test, test_pred, alpha=0.6, s=20, color='red', label='Test')\n",
    "ax3.plot([0, 6], [0, 6], 'r--', label='Perfect Prediction')\n",
    "ax3.set_xlabel('True Bandgap (eV)')\n",
    "ax3.set_ylabel('Predicted Bandgap (eV)')\n",
    "ax3.set_title(f'Test Predictions (R¬≤ = {test_r2:.3f})')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Residuals analysis\n",
    "test_residuals = y_test - test_pred\n",
    "ax4.scatter(test_pred, test_residuals, alpha=0.6, s=20, color='green')\n",
    "ax4.axhline(y=0, color='red', linestyle='--')\n",
    "ax4.set_xlabel('Predicted Bandgap (eV)')\n",
    "ax4.set_ylabel('Residuals (eV)')\n",
    "ax4.set_title('Residuals Analysis (Test Set)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Neural Network Insights:\")\n",
    "print(f\"‚Ä¢ The network learned structure-property relationships from {len(X_train)} training examples\")\n",
    "print(f\"‚Ä¢ Test R¬≤ of {test_r2:.3f} shows good generalization to unseen materials\")\n",
    "print(f\"‚Ä¢ Average prediction error of {test_mae:.3f} eV is reasonable for materials screening\")\n",
    "print(f\"‚Ä¢ The model can now predict bandgaps for new material compositions\")"
   ]
  }
 ]
}