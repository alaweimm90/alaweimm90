# =============================================================================
# PROMPT SELECTOR ENGINE v1.2
# Intelligent selection and composition of superprompts based on task analysis
# =============================================================================

metadata:
  version: '1.2.0'
  created: '2025-12-02'
  updated: '2025-12-03'
  purpose: 'Route tasks to appropriate superprompts and compose guidance'

# =============================================================================
# AVAILABLE SUPERPROMPTS
# =============================================================================

available_superprompts:
  # Core Superprompts
  - id: codebase-sentinel
    path: '../superprompts/codebase-sentinel.yaml'
    specialty: 'Code quality, governance, 7 Laws of Codebase Integrity'
    tier: core

  - id: security-auditor
    path: '../superprompts/security-auditor.yaml'
    specialty: 'OWASP Top 10, secret detection, security best practices'
    tier: core

  - id: architect
    path: '../superprompts/architect.yaml'
    specialty: 'System design, patterns, architectural decisions'
    tier: core

  - id: refactoring-expert
    path: '../superprompts/refactoring-expert.yaml'
    specialty: 'Safe refactoring, code smells, behavior-preserving changes'
    tier: core

  - id: debugger
    path: '../superprompts/debugger.yaml'
    specialty: 'Bug hunting, root cause analysis, scientific debugging'
    tier: core

  # Master Superprompt (combines all patterns)
  - id: master-orchestrator
    path: '../superprompts/master-orchestrator.yaml'
    specialty: 'Enterprise-grade orchestration combining security, optimization, governance'
    tier: master
    composition:
      ['codebase-sentinel', 'security-auditor', 'architect', 'refactoring-expert', 'debugger']

# =============================================================================
# EXTERNAL PROMPT CATALOG (Reference Library)
# =============================================================================

external_catalog:
  enterprise:
    - path: 'automation/prompts/project/ENTERPRISE_AGENTIC_AI_SUPERPROMPT.md'
      specialty: 'Multi-layer caching, agentic orchestration'
    - path: 'automation/prompts/project/GOVERNANCE_COMPLIANCE_SUPERPROMPT.md'
      specialty: 'COBIT, ITIL, SOC2, GDPR compliance'
    - path: 'automation/prompts/project/SECURITY_CYBERSECURITY_SUPERPROMPT.md'
      specialty: 'Defense in depth, DevSecOps'

  optimization:
    - path: 'automation/prompts/project/PROMPT_OPTIMIZATION_SUPERPROMPT.md'
      specialty: 'Token efficiency, IDE-specific formatting'

  domain_specific:
    - path: 'automation/prompts/project/CICD_PIPELINE_SUPERPROMPT.md'
    - path: 'automation/prompts/project/MONOREPO_ARCHITECTURE_SUPERPROMPT.md'
    - path: 'automation/prompts/project/TESTING_QA_SUPERPROMPT.md'
    - path: 'automation/prompts/project/AI_ML_INTEGRATION_SUPERPROMPT.md'

  reference:
    - path: 'docs/ai-coding-tools/02-terminal-cli/aider/aider/coders/architect_prompts.py'
      note: "Aider's production-proven architect prompts"

# =============================================================================
# TASK CLASSIFICATION
# =============================================================================

task_classification:
  # Intent extraction from user queries
  intent_patterns:
    # Highest priority: Enterprise/complex tasks invoke master orchestrator
    enterprise:
      keywords:
        [
          'enterprise',
          'production',
          'mission-critical',
          'comprehensive',
          'complete',
          'full-stack',
          'end-to-end',
          'orchestrate',
        ]
      weight: 1.0
      prompts: ['master-orchestrator']

    security:
      keywords:
        [
          'security',
          'vulnerability',
          'hack',
          'exploit',
          'secret',
          'password',
          'auth',
          'encrypt',
          'injection',
          'xss',
          'csrf',
        ]
      weight: 1.0
      prompts: ['security-auditor']

    architecture:
      keywords:
        [
          'design',
          'architect',
          'scale',
          'pattern',
          'structure',
          'system',
          'microservice',
          'api',
          'database',
          'schema',
        ]
      weight: 0.9
      prompts: ['architect']

    refactoring:
      keywords:
        [
          'refactor',
          'extract',
          'rename',
          'move',
          'inline',
          'simplify',
          'clean',
          'smell',
          'decompose',
        ]
      weight: 0.85
      prompts: ['refactoring-expert']

    debugging:
      keywords:
        [
          'bug',
          'debug',
          'error',
          'crash',
          'broken',
          'failing',
          'issue',
          'problem',
          'exception',
          'traceback',
          'stack',
        ]
      weight: 0.85
      prompts: ['debugger']

    quality:
      keywords: ['quality', 'lint', 'debt', 'improve', 'cleanup', 'standards', 'consistency']
      weight: 0.8
      prompts: ['codebase-sentinel', 'refactoring-expert']

    governance:
      keywords: ['policy', 'governance', 'compliance', 'audit', 'validate', 'check', 'enforce']
      weight: 0.8
      prompts: ['codebase-sentinel', 'security-auditor']

    documentation:
      keywords: ['document', 'readme', 'docs', 'explain', 'comment', 'describe', 'api-docs']
      weight: 0.7
      prompts: ['codebase-sentinel']

    fixing:
      keywords: ['fix', 'repair', 'resolve', 'patch', 'correct', 'address']
      weight: 0.75
      prompts: ['debugger', 'codebase-sentinel']

    implementation:
      keywords: ['implement', 'build', 'create', 'develop', 'code', 'feature', 'add']
      weight: 0.6
      prompts: ['architect', 'codebase-sentinel']

# =============================================================================
# SCORING ALGORITHM
# =============================================================================

scoring:
  algorithm: 'weighted_keyword_match'

  formula: |
    score = sum(
      keyword_matches[prompt] * intent_weight[intent] * recency_boost
    ) / normalization_factor

  thresholds:
    primary_selection: 0.6 # Use prompt if score >= 0.6
    secondary_inclusion: 0.3 # Include in composition if score >= 0.3
    minimum_match: 0.1 # Ignore if score < 0.1

  boosts:
    explicit_request: 2.0 # User explicitly asked for this type
    recent_success: 1.2 # This prompt succeeded recently
    domain_match: 1.5 # Task domain matches prompt specialty

  penalties:
    recent_failure: 0.7 # This prompt failed recently
    overuse: 0.9 # Avoid over-relying on one prompt

# =============================================================================
# COMPOSITION RULES
# =============================================================================

composition:
  # How to combine multiple prompts
  strategies:
    sequential:
      description: 'Apply prompts in order, each building on previous'
      when: 'Tasks with clear phases (analyze -> design -> implement)'
      example: ['codebase-sentinel.OBSERVE', 'architect.design', 'codebase-sentinel.VALIDATE']

    parallel:
      description: 'Apply prompts simultaneously, merge findings'
      when: 'Independent concerns that can be evaluated together'
      example: ['security-auditor.scan', 'codebase-sentinel.lint']

    hierarchical:
      description: 'Use one prompt as primary, others as advisors'
      when: 'Clear primary concern with secondary considerations'
      example:
        primary: 'architect'
        advisors: ['security-auditor', 'codebase-sentinel']

  # Conflict resolution when prompts disagree
  conflict_resolution:
    priority_order:
      - 'security' # Security concerns always win
      - 'correctness' # Then correctness
      - 'maintainability'
      - 'performance'
      - 'convenience'

    merge_strategy: |
      1. Identify conflicting recommendations
      2. Apply priority order to resolve
      3. Document the trade-off made
      4. Flag for human review if stakes are high

# =============================================================================
# CONTEXT EXTRACTION
# =============================================================================

context_extraction:
  from_query:
    - 'file_paths_mentioned'
    - 'technology_stack'
    - 'urgency_indicators'
    - 'scope_indicators'

  from_codebase:
    - 'primary_language'
    - 'framework_used'
    - 'existing_patterns'
    - 'recent_changes'

  from_history:
    - 'similar_past_tasks'
    - 'prompt_effectiveness'
    - 'user_preferences'

# =============================================================================
# SELECTION LOGIC (Pseudocode)
# =============================================================================

selection_logic: |
  function select_prompts(task: Task, context: Context) -> PromptSet {
    // Step 1: Extract intent
    intent = extract_intent(task.query)

    // Step 2: Score each prompt
    scores = {}
    for prompt in available_prompts:
      base_score = calculate_keyword_match(task.query, prompt.triggers)
      boosted_score = apply_boosts(base_score, context, prompt)
      penalized_score = apply_penalties(boosted_score, history, prompt)
      scores[prompt] = penalized_score

    // Step 3: Select prompts above threshold
    primary = max(scores)
    secondary = filter(scores, score >= SECONDARY_THRESHOLD)

    // Step 4: Determine composition strategy
    if has_clear_phases(task):
      strategy = "sequential"
    elif concerns_are_independent(primary, secondary):
      strategy = "parallel"
    else:
      strategy = "hierarchical"

    // Step 5: Compose and return
    return PromptSet(
      primary: primary,
      secondary: secondary,
      strategy: strategy,
      composition: compose(primary, secondary, strategy)
    )
  }

# =============================================================================
# API
# =============================================================================

api:
  select:
    input:
      query: "string - user's task description"
      context: 'object - optional context override'
    output:
      prompts: 'array - selected prompt IDs'
      composition: 'string - composed guidance'
      confidence: 'float - selection confidence'
      reasoning: 'string - why these prompts were selected'

  evaluate:
    input:
      task_id: 'string - completed task ID'
      outcome: 'enum - success | partial | failure'
      feedback: 'string - optional feedback'
    output:
      recorded: 'boolean'
      adjustments: 'object - any scoring adjustments made'

# =============================================================================
# EXAMPLES
# =============================================================================

examples:
  - query: 'Audit this codebase for security issues and code quality'
    selected:
      - 'security-auditor'
      - 'codebase-sentinel'
    strategy: 'parallel'
    confidence: 0.92
    reasoning: "Keywords 'audit', 'security', 'code quality' strongly match both prompts"

  - query: 'Design a new microservice architecture for user authentication'
    selected:
      - 'architect'
      - 'security-auditor'
    strategy: 'hierarchical'
    confidence: 0.88
    reasoning: 'Primary architecture task with security as critical advisor'

  - query: 'Fix this bug in the payment processing'
    selected:
      - 'debugger'
      - 'security-auditor'
    strategy: 'sequential'
    confidence: 0.85
    reasoning: "Debug methodology for bug with security due to 'payment'"

  - query: 'Refactor this large function into smaller pieces'
    selected:
      - 'refactoring-expert'
    strategy: 'single'
    confidence: 0.95
    reasoning: 'Direct refactoring task - extract_function from catalog'

  - query: 'The app crashes when submitting the form'
    selected:
      - 'debugger'
      - 'codebase-sentinel'
    strategy: 'sequential'
    confidence: 0.88
    reasoning: 'Crash investigation requires debugger methodology first'

  - query: 'Clean up the code smells in the user service'
    selected:
      - 'refactoring-expert'
      - 'codebase-sentinel'
    strategy: 'hierarchical'
    confidence: 0.90
    reasoning: 'Refactoring for smells with sentinel for quality gates'

  - query: 'Debug why tests are failing intermittently'
    selected:
      - 'debugger'
    strategy: 'single'
    confidence: 0.92
    reasoning: 'Intermittent failure - classic debugging scenario (race conditions)'
