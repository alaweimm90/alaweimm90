#!/usr/bin/env python3
"""
ML Prediction Pipeline
Generated by: mh predict init

Scikit-learn based prediction pipeline with training, evaluation, and inference.
"""

import json
import pickle
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    mean_squared_error,
    r2_score,
)


class PredictionPipeline:
    """ML Prediction Pipeline with training and inference support."""

    def __init__(self, model_dir: str = "./models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.model = None
        self.scaler = None
        self.label_encoders: Dict[str, LabelEncoder] = {}
        self.feature_names: List[str] = []
        self.target_name: str = ""
        self.model_type: str = "classification"  # or "regression"
        self.metadata: Dict[str, Any] = {}

    def load_data(
        self, filepath: str, target_column: str, feature_columns: Optional[List[str]] = None
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """Load and prepare data for training."""
        df = pd.read_csv(filepath)

        if feature_columns:
            X = df[feature_columns]
            self.feature_names = feature_columns
        else:
            X = df.drop(columns=[target_column])
            self.feature_names = X.columns.tolist()

        y = df[target_column]
        self.target_name = target_column

        return X, y

    def preprocess(
        self, X: pd.DataFrame, y: Optional[pd.Series] = None, fit: bool = True
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """Preprocess features and optionally target."""
        X_processed = X.copy()

        # Encode categorical columns
        for col in X_processed.select_dtypes(include=["object", "category"]).columns:
            if fit:
                self.label_encoders[col] = LabelEncoder()
                X_processed[col] = self.label_encoders[col].fit_transform(X_processed[col])
            else:
                X_processed[col] = self.label_encoders[col].transform(X_processed[col])

        # Scale numerical features
        if fit:
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X_processed)
        else:
            X_scaled = self.scaler.transform(X_processed)

        # Process target if provided
        y_processed = None
        if y is not None:
            if y.dtype == "object":
                if fit:
                    self.label_encoders["_target"] = LabelEncoder()
                    y_processed = self.label_encoders["_target"].fit_transform(y)
                else:
                    y_processed = self.label_encoders["_target"].transform(y)
            else:
                y_processed = y.values

        return X_scaled, y_processed

    def train(
        self,
        X: np.ndarray,
        y: np.ndarray,
        model_class: Any,
        model_params: Optional[Dict[str, Any]] = None,
        test_size: float = 0.2,
        cv_folds: int = 5,
    ) -> Dict[str, Any]:
        """Train model with cross-validation."""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )

        # Initialize model
        params = model_params or {}
        self.model = model_class(**params)

        # Cross-validation
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=cv_folds)

        # Train on full training set
        self.model.fit(X_train, y_train)

        # Evaluate
        y_pred = self.model.predict(X_test)
        metrics = self._calculate_metrics(y_test, y_pred)

        self.metadata = {
            "model_class": model_class.__name__,
            "model_params": params,
            "feature_names": self.feature_names,
            "target_name": self.target_name,
            "cv_scores": cv_scores.tolist(),
            "cv_mean": float(cv_scores.mean()),
            "cv_std": float(cv_scores.std()),
            "test_metrics": metrics,
            "trained_at": datetime.utcnow().isoformat(),
        }

        return self.metadata

    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """Calculate appropriate metrics based on model type."""
        if self.model_type == "classification":
            return {
                "accuracy": float(accuracy_score(y_true, y_pred)),
                "precision": float(precision_score(y_true, y_pred, average="weighted", zero_division=0)),
                "recall": float(recall_score(y_true, y_pred, average="weighted", zero_division=0)),
                "f1": float(f1_score(y_true, y_pred, average="weighted", zero_division=0)),
            }
        else:
            return {
                "mse": float(mean_squared_error(y_true, y_pred)),
                "rmse": float(np.sqrt(mean_squared_error(y_true, y_pred))),
                "r2": float(r2_score(y_true, y_pred)),
            }

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Make predictions on new data."""
        if self.model is None:
            raise ValueError("Model not trained. Call train() or load() first.")

        X_processed, _ = self.preprocess(X, fit=False)
        predictions = self.model.predict(X_processed)

        # Decode if classification with label encoder
        if "_target" in self.label_encoders:
            predictions = self.label_encoders["_target"].inverse_transform(predictions)

        return predictions

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Get prediction probabilities (classification only)."""
        if not hasattr(self.model, "predict_proba"):
            raise ValueError("Model does not support probability predictions")

        X_processed, _ = self.preprocess(X, fit=False)
        return self.model.predict_proba(X_processed)

    def save(self, name: str) -> Path:
        """Save model and preprocessing artifacts."""
        model_path = self.model_dir / f"{name}.pkl"
        metadata_path = self.model_dir / f"{name}_metadata.json"

        # Save model and preprocessors
        artifacts = {
            "model": self.model,
            "scaler": self.scaler,
            "label_encoders": self.label_encoders,
            "feature_names": self.feature_names,
            "target_name": self.target_name,
            "model_type": self.model_type,
        }

        with open(model_path, "wb") as f:
            pickle.dump(artifacts, f)

        with open(metadata_path, "w") as f:
            json.dump(self.metadata, f, indent=2)

        return model_path

    def load(self, name: str) -> "PredictionPipeline":
        """Load model and preprocessing artifacts."""
        model_path = self.model_dir / f"{name}.pkl"
        metadata_path = self.model_dir / f"{name}_metadata.json"

        with open(model_path, "rb") as f:
            artifacts = pickle.load(f)

        self.model = artifacts["model"]
        self.scaler = artifacts["scaler"]
        self.label_encoders = artifacts["label_encoders"]
        self.feature_names = artifacts["feature_names"]
        self.target_name = artifacts["target_name"]
        self.model_type = artifacts["model_type"]

        with open(metadata_path, "r") as f:
            self.metadata = json.load(f)

        return self


# Example usage
if __name__ == "__main__":
    from sklearn.ensemble import RandomForestClassifier

    # Initialize pipeline
    pipeline = PredictionPipeline(model_dir="./models")

    # Load data
    # X, y = pipeline.load_data("data.csv", target_column="target")

    # Preprocess
    # X_processed, y_processed = pipeline.preprocess(X, y, fit=True)

    # Train
    # metrics = pipeline.train(
    #     X_processed, y_processed,
    #     model_class=RandomForestClassifier,
    #     model_params={"n_estimators": 100, "max_depth": 10}
    # )

    # Save
    # pipeline.save("my_model")

    # Load and predict
    # pipeline.load("my_model")
    # predictions = pipeline.predict(new_data)

    print("PredictionPipeline ready. See example usage above.")
