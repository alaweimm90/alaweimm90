# Real-World DevOps Workflow Examples

Complete end-to-end examples using the autonomous DevOps MCP system.

---

## Example 1: Full-Stack Feature Deployment

### Scenario
Deploy a new user authentication feature with complete error-free validation.

### Workflow Execution
```bash
cd /mnt/c/Users/mesha/Desktop/GitHub

python .metaHub/scripts/devops_workflow_runner.py \
  --problem "Deploy user authentication feature with OAuth2 and 2FA" \
  --workspace $(pwd)
```

### Pipeline Stages

#### Stage 1: Sequential Thinking Analysis
**MCP:** `sequential-thinking`
**Output:**
```
Problem Decomposition:
1. Analyze authentication requirements (OAuth2 + 2FA)
2. Design database schema for user credentials
3. Implement OAuth2 flow (authorization code grant)
4. Add 2FA with TOTP (Time-based One-Time Password)
5. Write comprehensive tests (unit, integration, E2E)
6. Deploy infrastructure with Terraform
7. Monitor authentication success rate with Prometheus

Critical Points:
- Ensure secure password hashing (bcrypt with cost factor 12)
- Store 2FA secrets encrypted in database
- Test OAuth2 flows for all providers (Google, GitHub, Microsoft)
- Validate TOTP implementation against RFC 6238
- Monitor failed authentication attempts (rate limiting)
```

#### Stage 2: Git State Analysis
**MCP:** `git`
**Output:**
```
Branch: feature/oauth2-2fa-authentication
Modified Files: 12
  - backend/src/auth/oauth2.ts (new)
  - backend/src/auth/totp.ts (new)
  - backend/src/models/user.ts (modified)
  - backend/src/api/auth.routes.ts (modified)
  - frontend/src/components/LoginForm.tsx (modified)
  - frontend/src/components/TwoFactorSetup.tsx (new)
  - database/migrations/001_add_auth_tables.sql (new)
  - tests/auth/oauth2.test.ts (new)
  - tests/auth/totp.test.ts (new)
  - tests/e2e/auth-flow.spec.ts (new)
  - terraform/auth-service.tf (new)
  - docs/AUTHENTICATION.md (new)

Recent Commits:
- feat: implement OAuth2 authorization code flow
- feat: add TOTP 2FA support
- test: add comprehensive auth tests
- infra: add auth service infrastructure
```

#### Stage 3: Playwright Testing
**MCP:** `playwright`
**Test Suites:**
```
E2E Authentication Flow Tests:
‚úÖ User can sign up with OAuth2 (Google)
‚úÖ User can sign up with OAuth2 (GitHub)
‚úÖ User receives email verification
‚úÖ User can set up 2FA with QR code
‚úÖ User can log in with TOTP code
‚úÖ Failed TOTP attempts are rate-limited
‚úÖ Session cookies are secure and httpOnly
‚úÖ CSRF protection is enabled

Results: 8/8 tests passed (Duration: 45s)
```

#### Stage 4: Security Scanning
**MCP:** `semgrep`
**Security Checks:**
```
SAST Analysis Results:
‚úÖ No SQL injection vulnerabilities
‚úÖ No XSS vulnerabilities
‚úÖ Secure password hashing (bcrypt detected)
‚úÖ CSRF tokens validated
‚úÖ Session management secure
‚úÖ No hardcoded secrets detected
‚úÖ OWASP Top 10 compliance: 100%

Vulnerabilities Found: 0 critical, 0 high, 0 medium
```

#### Stage 5: Terraform Infrastructure
**MCP:** `terraform`
**Infrastructure Changes:**
```
Terraform Plan:
  + aws_rds_instance.auth_db
      instance_class: db.t3.micro
      encrypted: true
      backup_retention: 7 days

  + aws_elasticache_cluster.session_store
      engine: redis
      node_type: cache.t3.micro

  + aws_secretsmanager_secret.oauth_credentials
      encrypted: true

  ~ aws_ecs_service.api
      desired_count: 2 ‚Üí 3 (authentication load)

Changes: +3 to add, ~1 to change, 0 to destroy
Estimated Cost: +$45/month
```

#### Stage 6: Kubernetes Deployment
**MCP:** `kubernetes`
**Deployment:**
```
Kubernetes Resources:
‚úÖ Deployment: auth-service (3 replicas)
‚úÖ Service: auth-service-lb (LoadBalancer)
‚úÖ ConfigMap: auth-config
‚úÖ Secret: oauth-credentials (sealed)
‚úÖ HorizontalPodAutoscaler: auth-service-hpa
   Min Replicas: 3
   Max Replicas: 10
   Target CPU: 70%

Health Check: All pods healthy (3/3)
```

#### Stage 7: Prometheus Monitoring
**MCP:** `prometheus`
**Metrics:**
```
Authentication Metrics (Last 5 min):
- auth_login_success_total: 45
- auth_login_failed_total: 2
- auth_2fa_enabled_total: 38
- auth_oauth2_callback_duration_seconds: p95=0.35s
- auth_totp_validation_duration_seconds: p95=0.02s

Health Status: ‚úÖ Healthy
Success Rate: 95.7% (target: >95%)
P95 Latency: 350ms (target: <500ms)
```

### Final Result
```
‚úÖ DEPLOYMENT COMPLETE
- Feature: User Authentication (OAuth2 + 2FA)
- Tests: 8/8 passed
- Security: 0 vulnerabilities
- Infrastructure: Deployed (3 new resources)
- Monitoring: Healthy (95.7% success rate)
- Total Duration: 8 minutes 23 seconds
```

---

## Example 2: Bug Fix with Root Cause Analysis

### Scenario
Production incident: Users reporting intermittent 500 errors on checkout page.

### Workflow Execution
```bash
python .metaHub/scripts/devops_workflow_runner.py \
  --problem "Debug and fix intermittent 500 errors on checkout" \
  --workspace $(pwd)
```

### Pipeline Stages

#### Stage 1: Sequential Thinking Root Cause Analysis
**MCP:** `sequential-thinking`
**Analysis:**
```
Problem: Intermittent 500 errors on checkout
Hypothesis Generation:
1. Database connection pool exhaustion?
2. Race condition in payment processing?
3. Timeout in external payment API?
4. Memory leak in checkout service?

Step-by-Step Investigation:
1. Check Prometheus for error spike timing
2. Analyze Git commit history for recent changes
3. Review application logs for error patterns
4. Test payment flow with Playwright to reproduce
5. Validate database connection pool settings
6. Profile checkout service memory usage

Root Cause Found:
- Payment API timeout (30s) too short for Stripe webhook validation
- Stripe webhooks taking 35-45s during high load
- Checkout service not handling async webhook correctly
```

#### Stage 2: Prometheus Investigation
**MCP:** `prometheus`
**Metrics Analysis:**
```
Error Spike Analysis:
- Timestamp: 2025-11-28 14:35:00 UTC
- Duration: 15 minutes
- Affected Requests: 347/2,451 (14.2%)

PromQL Queries:
rate(checkout_errors_total[5m])
  Result: Spike to 4.5 errors/sec (baseline: 0.1/sec)

histogram_quantile(0.99, checkout_duration_seconds)
  Result: P99 = 42.3s (normal: 2.1s)

payment_webhook_timeout_total
  Result: 347 timeouts (correlates with errors)
```

#### Stage 3: Git Commit Analysis
**MCP:** `git`
**Recent Changes:**
```
Suspicious Commit:
commit 7f3a9b2
Author: developer@example.com
Date: 2025-11-27 16:30:00

feat: migrate payment processing to async webhooks

Changed Files:
- services/checkout/src/payment-processor.ts
  Old: Synchronous Stripe charge confirmation
  New: Async webhook-based confirmation

Issue: Webhook timeout not increased from 30s ‚Üí 60s
```

#### Stage 4: Playwright Reproduction
**MCP:** `playwright`
**Test:**
```
Reproduction Test:
‚úÖ Navigate to checkout page
‚úÖ Add test product to cart
‚úÖ Enter payment details
‚ùå Submit payment (timeout after 30s)

Error Captured:
  PaymentTimeoutError: Webhook confirmation timed out
  Expected: <30s, Actual: 42s

Reproduction Success: 10/10 attempts
```

#### Stage 5: Fix Implementation & Testing
**MCP:** `git` + `playwright`
**Fix:**
```
git diff services/checkout/src/payment-processor.ts

- const WEBHOOK_TIMEOUT = 30000; // 30 seconds
+ const WEBHOOK_TIMEOUT = 60000; // 60 seconds

- await webhookConfirmation.wait({ timeout: 30000 });
+ await webhookConfirmation.wait({ timeout: 60000 });

Playwright Re-test:
‚úÖ Payment submission successful (avg: 38s)
‚úÖ Webhook confirmation received
‚úÖ Order created successfully
Success Rate: 10/10 attempts
```

#### Stage 6: Deploy Fix
**MCP:** `kubernetes`
**Deployment:**
```
Rolling Update:
‚úÖ Updated ConfigMap: payment-config
   WEBHOOK_TIMEOUT: 60000

‚úÖ Deployment rollout: checkout-service
   Strategy: RollingUpdate (25% max surge)
   Rollout Status: 4/4 pods updated

Health Check: All pods healthy
```

#### Stage 7: Validation
**MCP:** `prometheus`
**Post-Deployment:**
```
Error Rate (Last 15 min):
- checkout_errors_total: 0.08/sec (back to baseline)
- payment_webhook_timeout_total: 0 (no timeouts)
- checkout_success_rate: 99.8%

Incident Resolved: ‚úÖ
MTTR (Mean Time To Resolution): 23 minutes
```

### Final Result
```
‚úÖ BUG FIX COMPLETE
- Root Cause: Webhook timeout too short (30s ‚Üí 60s)
- Fix: Increased timeout to 60 seconds
- Testing: 100% success rate (10/10)
- Deployment: Rolling update successful
- Validation: Error rate back to baseline
- MTTR: 23 minutes (Target: <30 min) ‚úÖ
```

---

## Example 3: Infrastructure Scaling

### Scenario
Black Friday traffic spike expected - scale infrastructure proactively.

### Workflow Execution
```bash
python .metaHub/scripts/devops_workflow_runner.py \
  --problem "Scale infrastructure for Black Friday 10x traffic" \
  --workspace $(pwd)
```

### Pipeline Stages

#### Stage 1: Capacity Planning
**MCP:** `sequential-thinking`
**Analysis:**
```
Current Capacity:
- API Servers: 5 instances (t3.medium)
- Database: RDS db.t3.large (max connections: 100)
- Cache: Redis cluster (3 nodes, 2GB each)
- CDN: CloudFront (standard tier)

Expected Black Friday Load:
- Normal: 1,000 req/min
- Black Friday: 10,000 req/min (10x)

Scaling Plan:
1. API Servers: 5 ‚Üí 20 instances (4x with buffer)
2. Database: db.t3.large ‚Üí db.r5.xlarge (4x capacity)
3. Cache: 3 nodes ‚Üí 6 nodes (2x capacity)
4. Database Read Replicas: 1 ‚Üí 3 (distribute read load)
5. CDN: Enable origin shield + aggressive caching
```

#### Stage 2: Terraform Infrastructure Changes
**MCP:** `terraform`
**Plan:**
```
Terraform Plan:
  ~ aws_autoscaling_group.api_servers
      min_size: 5 ‚Üí 10
      max_size: 10 ‚Üí 30
      desired_capacity: 5 ‚Üí 20

  ~ aws_db_instance.main
      instance_class: db.t3.large ‚Üí db.r5.xlarge
      iops: 3000 ‚Üí 12000

  + aws_db_instance.read_replica_2 (new)
  + aws_db_instance.read_replica_3 (new)

  ~ aws_elasticache_replication_group.redis
      number_cache_clusters: 3 ‚Üí 6

  + aws_cloudfront_origin_access_control.oac (new)

Changes: +4 to add, ~3 to change
Estimated Cost: $450/month ‚Üí $1,850/month (Black Friday period)
```

#### Stage 3: Kubernetes Scaling
**MCP:** `kubernetes`
**Scaling:**
```
HorizontalPodAutoscaler Updates:
  ~ api-service-hpa
      minReplicas: 5 ‚Üí 15
      maxReplicas: 10 ‚Üí 40
      targetCPUUtilizationPercentage: 70 ‚Üí 60

  ~ worker-service-hpa
      minReplicas: 3 ‚Üí 10
      maxReplicas: 8 ‚Üí 25

PodDisruptionBudget Updates:
  ~ api-service-pdb
      minAvailable: 3 ‚Üí 10 (ensure availability during scale)

Current Status:
‚úÖ API Pods: 15/15 ready
‚úÖ Worker Pods: 10/10 ready
‚úÖ All pods passed health checks
```

#### Stage 4: Load Testing
**MCP:** `playwright`
**Synthetic Load Test:**
```
Simulated Black Friday Traffic:
- Virtual Users: 1,000 concurrent
- Duration: 10 minutes
- Ramp-up: 2 minutes

Results:
‚úÖ Success Rate: 99.9%
‚úÖ P95 Response Time: 285ms (target: <500ms)
‚úÖ P99 Response Time: 450ms (target: <1s)
‚úÖ Error Rate: 0.1% (target: <1%)
‚úÖ No database connection errors
‚úÖ No cache evictions under load

Capacity Validated: ‚úÖ Ready for 10x traffic
```

#### Stage 5: Monitoring Setup
**MCP:** `prometheus`
**Alerts Configured:**
```
New Prometheus Alerts:
- HighTrafficAlert: req/min > 8,000 (80% of capacity)
- DatabaseConnectionsHigh: connections > 320 (80% of max)
- CacheHitRateLow: cache_hit_rate < 85%
- APILatencyHigh: p95_latency > 800ms

Dashboard Created:
- Black Friday Real-time Dashboard
- Auto-scaling metrics
- Cost monitoring
- Capacity utilization
```

### Final Result
```
‚úÖ INFRASTRUCTURE SCALED
- API Capacity: 5 ‚Üí 20 instances (4x)
- Database: Upgraded + 2 read replicas
- Cache: 3 ‚Üí 6 nodes (2x)
- Load Test: 99.9% success at 10x traffic
- Cost: $1,850/month (Black Friday period)
- Monitoring: Real-time dashboard + alerts
- Status: Ready for Black Friday ‚úÖ
```

---

## Example 4: Multi-Agent Research Workflow (MeatheadPhysicist)

### Scenario
Literature review and analysis for quantum computing research paper.

### Agent Workflow
```bash
# This workflow uses MeatheadPhysicist agents with MCP integrations
# Coordinated by: MeatheadPhysicist Orchestrator
```

### Multi-Agent Pipeline

#### Agent 1: ScoutAgent
**MCPs Used:** `brave_search`, `context`
```
Task: Find relevant quantum computing papers
Search Queries:
- "quantum error correction 2024"
- "surface code improvements"
- "fault-tolerant quantum computing"

Results Found: 47 papers
Filtered: 12 highly relevant papers
Stored in Context MCP for team access
```

#### Agent 2: LiteratureAgent
**MCPs Used:** `brave_search`, `git`, `filesystem`, `context`
```
Task: Detailed analysis of 12 papers

For Each Paper:
1. Download PDF via Brave Search
2. Extract key findings
3. Identify novel contributions
4. Track in Git (literature/quantum-error-correction/)
5. Update shared context

Output:
- 12 paper summaries (saved to filesystem)
- Bibliography entries (BibTeX format)
- Cross-reference matrix (identifies connections)
- Version controlled in Git
```

#### Agent 3: TheoryAgent
**MCPs Used:** `sequential_thinking`, `filesystem`, `git`, `context`
```
Task: Develop theoretical framework

Sequential Thinking Analysis:
1. Identify common themes across 12 papers
2. Synthesize unified theoretical framework
3. Derive mathematical relationships
4. Validate consistency with existing theories

Output:
- Theoretical framework document (LaTeX)
- Mathematical derivations (proof-checked)
- Saved to filesystem + Git version control
- Shared via Context MCP
```

#### Agent 4: VisualizationAgent
**MCPs Used:** `filesystem`, `git`, `context`
```
Task: Create publication-ready figures

Visualizations Created:
1. Error rate comparison (12 papers)
2. Theoretical framework diagram
3. Performance scaling plots
4. Timeline of improvements

Output:
- 8 high-resolution figures (PNG, SVG)
- Figure source code (Python/matplotlib)
- Saved to filesystem
- Version controlled in Git
```

#### Agent 5: CriticAgent
**MCPs Used:** `sequential_thinking`, `context`, `git`
```
Task: Critical review of theoretical framework

Sequential Thinking Review:
1. Check mathematical consistency
2. Identify potential flaws
3. Suggest improvements
4. Validate experimental predictions

Findings:
- 3 minor inconsistencies identified
- 2 improvement suggestions
- 1 additional experiment proposed
- Review saved to Git
```

### Final Research Output
```
‚úÖ RESEARCH WORKFLOW COMPLETE
- Papers Analyzed: 12 (from 47 candidates)
- Theoretical Framework: Developed & validated
- Figures: 8 publication-ready visualizations
- Critical Review: 3 issues addressed
- Git Commits: 47 (full version history)
- Shared Context: All agents synchronized
- Duration: 4 hours (automated)
- Next Step: Draft paper introduction
```

---

## Dashboard Visualization

View workflow telemetry with:
```bash
python .metaHub/scripts/telemetry_dashboard.py
```

Expected Output:
```
================================================================================
üîç MCP TELEMETRY DASHBOARD
================================================================================
üìä SYSTEM HEALTH
  Latest Workflow: ‚úÖ SUCCESS
  Workflow ID: workflow_20251128_145623
  Steps Completed: 18
  Errors: 0

üöÄ WORKFLOW PIPELINE
  ‚úÖ Analysis: SUCCESS
  ‚úÖ Git State: SUCCESS
  ‚úÖ Tests: 8/8 passed
  ‚úÖ Infrastructure: +3 ~1 -0
  ‚úÖ Deployment: 3/3 pods healthy
  ‚úÖ Monitoring: 95.7% success rate
================================================================================
```

---

## Summary

These real-world examples demonstrate:
1. **Full-stack feature deployment** with complete validation
2. **Bug fixing** with root cause analysis
3. **Infrastructure scaling** for traffic spikes
4. **Multi-agent research** workflows

All workflows are **autonomous, error-free, and fully monitored** using the MCP orchestration system.
